{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataloaders for lstm_only model\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from convert import read_mm\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data(data, info, split):\n",
    "    \"\"\"Slice data according to the instances belonging to each split.\"\"\"\n",
    "    if split is None:\n",
    "        return data\n",
    "    split_indices = {\n",
    "        'train': slice(0, info['train_len']),\n",
    "        'val': slice(info['train_len'], info['train_len'] + info['val_len']),\n",
    "        'test': slice(info['train_len'] + info['val_len'], info['total'])\n",
    "    }\n",
    "    return data[split_indices[split]]  # return the sliced data, eg. data[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_mask_cols(ts_info, seq):\n",
    "    \"\"\"Remove temporal mask columns.\"\"\"\n",
    "    neg_mask_cols = [i for i, e in enumerate(ts_info['columns']) if 'mask' not in e]\n",
    "    return seq[:, :, neg_mask_cols] # return the sequence without mask columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ts_flat_labels(data_dir, ts_mask, task, add_diag, split=None, debug=0, split_flat_and_diag=False):\n",
    "    \"\"\"Read temporal, flat data and task labels.\"\"\"\n",
    "    def read_and_slice(name):\n",
    "        data, info = read_mm(data_dir, name)\n",
    "        return slice_data(data, info, split), info\n",
    "\n",
    "    flat, flat_info = read_and_slice('flat')\n",
    "    seq, ts_info = read_and_slice('ts')\n",
    "    if not ts_mask:\n",
    "        seq = no_mask_cols(ts_info, seq)\n",
    "\n",
    "    if add_diag:\n",
    "        diag, _ = read_and_slice('diagnoses')\n",
    "        flat = (flat, diag) if split_flat_and_diag else np.concatenate([flat, diag], axis=1)\n",
    "\n",
    "    labels, _ = read_and_slice('labels')\n",
    "    labels = labels[:, {'ihm': 2, 'multi': [1,2,4]}[task]]\n",
    "\n",
    "    if debug:\n",
    "        N = 1000\n",
    "        train_n, val_n = int(N * 0.5), int(N * 0.25)\n",
    "    else:\n",
    "        N, train_n, val_n = flat_info['total'], flat_info['train_len'], flat_info['val_len']\n",
    "\n",
    "    return seq[:N], flat[:N], labels[:N], flat_info, N, train_n, val_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(train_labels):\n",
    "    \"\"\"Return class weights to handle class imbalance problems.\"\"\"\n",
    "    occurences = np.unique(train_labels, return_counts=True)[1]\n",
    "    class_weights = torch.Tensor(occurences.sum() / occurences).float()\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LstmDataset(Dataset):\n",
    "    \"\"\"Dataset class for temporal data.\"\"\"\n",
    "    def __init__(self, config, split=None):\n",
    "        super().__init__()\n",
    "        task = config['task']\n",
    "        self.seq, self.flat, self.labels, self.ts_info, self.N, train_n, val_n = collect_ts_flat_labels(\n",
    "            config['data_dir'], config['ts_mask'], task, config['add_diag'], split, debug=0)\n",
    "\n",
    "        self.ts_dim, self.flat_dim = self.seq.shape[2], self.flat.shape[1]\n",
    "        self.split_n = {'train': train_n, 'val': val_n, 'test': self.N - train_n - val_n}.get(split, self.N)\n",
    "        self.ids = slice_data(np.arange(self.N), self.ts_info, split)\n",
    "        self.class_weights = get_class_weights(self.labels[:train_n]) if task == 'ihm' else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.split_n\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.seq[index], self.flat[index], self.labels[index], self.ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Collect samples in each batch.\"\"\"\n",
    "    seq, flat, labels, ids = zip(*batch)\n",
    "    seq = torch.Tensor(np.stack(seq)).float()\n",
    "    flat = torch.Tensor(np.stack(flat)).float()\n",
    "    labels = torch.Tensor(np.stack(labels)).float() if labels[0].dtype == np.float32 else torch.Tensor(np.stack(labels)).long()\n",
    "    ids = torch.Tensor(np.stack(ids)).long()\n",
    "    return (seq, flat), labels, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(config, split=\"train\", batch_size=32, shuffle=True):\n",
    "    dataset = LstmDataset(config, split)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eicu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
