{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphSAGE\n",
    "from minisom import MiniSom  \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mei/nas/docker/thesis')\n",
    "from dataloader.ts_reader import LSTMTSDataset,collate_fn\n",
    "from dataloader.pyg_reader import GraphDataset,get_graph_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalICUModel(nn.Module):\n",
    "    def __init__(self, flat_input_dim, ts_input_dim, hidden_dim, \n",
    "                 gnn_input_dim, gnn_hidden_dim, som_dim, num_classes):\n",
    "        super(MultiModalICUModel, self).__init__()\n",
    "\n",
    "        # **1️⃣ Flat Data → MLP**\n",
    "        self.fc_flat = nn.Sequential(\n",
    "            nn.Linear(flat_input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # **2️⃣ Time Series → LSTM**\n",
    "        self.lstm = nn.LSTM(input_size=ts_input_dim, hidden_size=hidden_dim, \n",
    "                            num_layers=2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # **3️⃣ Graph Data → GNN**\n",
    "        self.gnn = GraphSAGE(gnn_input_dim, gnn_hidden_dim, num_layers=2)  # ✅ 添加 num_layers=2\n",
    "        self.gnn_fc = nn.Linear(gnn_hidden_dim, hidden_dim)  # Graph embedding 维度调整\n",
    "\n",
    "        # **4️⃣ Self-Organizing Map (SOM)**\n",
    "        self.som_dim = som_dim  \n",
    "        self.som = MiniSom(som_dim, som_dim, input_len=hidden_dim * 3, sigma=0.3, learning_rate=0.5)\n",
    "        \n",
    "        # **5️⃣ Classification Head**\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.Linear(som_dim * som_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_classes)  \n",
    "        )\n",
    "\n",
    "    def forward(self, flat, ts, graph_data):\n",
    "        \"\"\"\n",
    "        flat: (batch_size, flat_input_dim)\n",
    "        ts: (batch_size, seq_len, ts_input_dim)\n",
    "        graph_data: PyG Data 对象\n",
    "        \"\"\"\n",
    "\n",
    "        # **1️⃣ Flat Data → Embedding**\n",
    "        flat_embed = self.fc_flat(flat)  \n",
    "\n",
    "        # **2️⃣ Time Series → LSTM**\n",
    "        lstm_out, _ = self.lstm(ts)  \n",
    "        lstm_embed = lstm_out[:, -1, :]  \n",
    "\n",
    "        # **3️⃣ Graph → GNN**\n",
    "        gnn_embed = self.gnn(graph_data.x, graph_data.edge_index)  \n",
    "        gnn_embed = self.gnn_fc(gnn_embed)  \n",
    "\n",
    "        # **4️⃣ Multi-modal Fusion**\n",
    "        fusion = torch.cat([flat_embed, lstm_embed, gnn_embed], dim=-1)  \n",
    "\n",
    "        # **5️⃣ SOM**\n",
    "        som_input = fusion.cpu().detach().numpy()  \n",
    "        som_output = np.array([self.som.winner(x) for x in som_input])  \n",
    "        som_output = som_output.reshape(fusion.shape[0], -1)  \n",
    "        som_output = torch.tensor(som_output, dtype=torch.float).to(flat.device)  \n",
    "\n",
    "        # **6️⃣ Classification**\n",
    "        output = self.clf(som_output)  \n",
    "\n",
    "        return output, fusion, som_output  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/mei/nas/docker/thesis/data/hdf/train\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {  \n",
    "    \"data_dir\": \"/home/mei/nas/docker/thesis/data/hdf/val\",\n",
    "    \"graph_dir\": \"/home/mei/nas/docker/thesis/data/graphs\",\n",
    "    \"mode\": \"k_closest\",\n",
    "    \"k\": 3\n",
    "          \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading precomputed graph from /home/mei/nas/docker/thesis/data/graphs/diagnosis_graph_k_closest_k3.pt\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Only slices (':'), list, tuples, torch.tensor and np.ndarray of dtype long or bool are valid indices (got 'str')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# === Graph Dataset ===\u001b[39;00m\n\u001b[1;32m      8\u001b[0m graph_dataset \u001b[38;5;241m=\u001b[39m GraphDataset(config)\n\u001b[0;32m----> 9\u001b[0m graph_loader \u001b[38;5;241m=\u001b[39m \u001b[43mget_graph_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/nas/docker/thesis/dataloader/pyg_reader.py:47\u001b[0m, in \u001b[0;36mget_graph_dataloader\u001b[0;34m(config, batch_size, shuffle)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_graph_dataloader\u001b[39m(config, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    Create PyG dataloader for training.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mGraphDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39mshuffle)\n",
      "File \u001b[0;32m~/nas/docker/thesis/dataloader/pyg_reader.py:15\u001b[0m, in \u001b[0;36mGraphDataset.__init__\u001b[0;34m(self, config, transform, pre_transform)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pre_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28msuper\u001b[39m(GraphDataset, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_dir\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, transform\u001b[38;5;241m=\u001b[39mtransform, pre_transform\u001b[38;5;241m=\u001b[39mpre_transform)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# === 1. Graph ===\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     graph_path \u001b[38;5;241m=\u001b[39m Path(config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagnosis_graph_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_k\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch_geometric/data/dataset.py:296\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch_geometric/data/dataset.py:339\u001b[0m, in \u001b[0;36mDataset.index_select\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    336\u001b[0m     indices \u001b[38;5;241m=\u001b[39m [indices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\n\u001b[1;32m    340\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly slices (\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m), list, tuples, torch.tensor and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    341\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray of dtype long or bool are valid indices (got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(idx)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    344\u001b[0m dataset \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    345\u001b[0m dataset\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;241m=\u001b[39m indices\n",
      "\u001b[0;31mIndexError\u001b[0m: Only slices (':'), list, tuples, torch.tensor and np.ndarray of dtype long or bool are valid indices (got 'str')"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# === LSTM + Flat Dataset ===\n",
    "lstm_dataset = LSTMTSDataset(data_dir)\n",
    "lstm_loader = DataLoader(lstm_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# === Graph Dataset ===\n",
    "graph_dataset = GraphDataset(config)\n",
    "graph_loader = get_graph_dataloader(graph_dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# **模型**\n",
    "model = MultiModalICUModel(\n",
    "    flat_input_dim=104, ts_input_dim=163, hidden_dim=128, \n",
    "    gnn_input_dim=128, gnn_hidden_dim=128, som_dim=10, num_classes=2\n",
    ").to(device)\n",
    "\n",
    "# **优化器 & 损失**\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# **获取完整图数据（因为 GNN 处理整张图）**\n",
    "graph_data = next(iter(graph_loader)).to(device)  # 只取一次完整的图\n",
    "\n",
    "# **训练循环**\n",
    "for epoch in range(10):\n",
    "    for batch in lstm_loader:\n",
    "        (seqs_padded, seq_lengths, flats), labels, ids = batch\n",
    "\n",
    "        # **移动数据到 GPU**\n",
    "        seqs_padded, seq_lengths = seqs_padded.to(device), seq_lengths.to(device)\n",
    "        flats, labels = flats.to(device), labels.to(device)\n",
    "\n",
    "        # **前向传播**\n",
    "        outputs, fusion, som_output = model(flats, seqs_padded, seq_lengths, graph_data)\n",
    "\n",
    "        # **计算损失**\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # **梯度下降**\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "som_output = som_out.cpu().detach().numpy()\n",
    "plt.scatter(som_output[:, 0], som_output[:, 1], c=labels.cpu().numpy(), cmap=\"coolwarm\")\n",
    "plt.title(\"SOM Feature Space\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eicu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
