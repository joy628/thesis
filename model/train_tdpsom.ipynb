{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-02 15:59:09.182032: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738508349.207647 3823560 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738508349.215583 3823560 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-02 15:59:09.243734: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mei/anaconda3/envs/eicu/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/mei/nas/docker/thesis')\n",
    "from model.tdpsom import TDPSOM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataloader.ts_reader import LSTMTSDataset,collate_fn\n",
    "from dataloader.pyg_reader import GraphDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 128\n",
    "som_dim = [24, 24]\n",
    "lstm_dim = 200\n",
    "dropout = 0.5\n",
    "learning_rate_val = 0.001\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "\n",
    "input_channels = 163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/mei/nas/docker/thesis/data/hdf/train\"\n",
    "config = {  \n",
    "    \"data_dir\": \"/home/mei/nas/docker/thesis/data/hdf\",\n",
    "    \"graph_dir\": \"/home/mei/nas/docker/thesis/data/graphs\",\n",
    "    \"mode\": \"k_closest\",\n",
    "    \"k\": 3\n",
    "          \n",
    "}\n",
    "\n",
    "lstm_dataset = LSTMTSDataset(data_dir,debug=True)\n",
    "lstm_loader = DataLoader(lstm_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738508376.671551 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21959 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:44:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508376.673880 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 21917 MB memory:  -> device: 1, name: Quadro RTX 6000, pci bus id: 0000:45:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508376.675637 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22805 MB memory:  -> device: 2, name: Quadro RTX 6000, pci bus id: 0000:46:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508376.677316 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 7 MB memory:  -> device: 3, name: Quadro RTX 6000, pci bus id: 0000:47:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508376.678905 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 6273 MB memory:  -> device: 4, name: Quadro RTX 6000, pci bus id: 0000:84:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508376.680472 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 568 MB memory:  -> device: 5, name: Quadro RTX 6000, pci bus id: 0000:85:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508376.682047 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 592 MB memory:  -> device: 6, name: Quadro RTX 6000, pci bus id: 0000:86:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508376.683659 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 22805 MB memory:  -> device: 7, name: Quadro RTX 6000, pci bus id: 0000:87:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = TDPSOM(input_size=input_channels, latent_dim=latent_dim, som_dim=som_dim,\n",
    "               learning_rate=learning_rate_val, dropout=dropout, input_channels=input_channels,\n",
    "               lstm_dim=lstm_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1738508383.327045 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21959 MB memory:  -> device: 0, name: Quadro RTX 6000, pci bus id: 0000:44:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508383.328386 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 21917 MB memory:  -> device: 1, name: Quadro RTX 6000, pci bus id: 0000:45:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508383.329513 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 22805 MB memory:  -> device: 2, name: Quadro RTX 6000, pci bus id: 0000:46:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508383.331162 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 7 MB memory:  -> device: 3, name: Quadro RTX 6000, pci bus id: 0000:47:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508383.332305 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:4 with 6273 MB memory:  -> device: 4, name: Quadro RTX 6000, pci bus id: 0000:84:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508383.333277 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:5 with 568 MB memory:  -> device: 5, name: Quadro RTX 6000, pci bus id: 0000:85:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508383.343670 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:6 with 592 MB memory:  -> device: 6, name: Quadro RTX 6000, pci bus id: 0000:86:00.0, compute capability: 7.5\n",
      "I0000 00:00:1738508383.345100 3823560 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:7 with 22805 MB memory:  -> device: 7, name: Quadro RTX 6000, pci bus id: 0000:87:00.0, compute capability: 7.5\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1738508383.393233 3823560 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss = nan\n",
      "Epoch 2: Average Loss = nan\n",
      "Epoch 3: Average Loss = nan\n",
      "Epoch 4: Average Loss = nan\n",
      "Epoch 5: Average Loss = nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m      8\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (flats, seqs_padded, seq_lengths), labels, ids \u001b[38;5;129;01min\u001b[39;00m lstm_loader:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# 将 PyTorch Tensor 转换为 numpy 数组\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# 注意：如果你的 tensor 在 GPU 上，则调用 .cpu().numpy()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     batch_data \u001b[38;5;241m=\u001b[39m seqs_padded\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mif\u001b[39;00m seqs_padded\u001b[38;5;241m.\u001b[39mis_cuda \u001b[38;5;28;01melse\u001b[39;00m seqs_padded\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     14\u001b[0m     feed_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m         model\u001b[38;5;241m.\u001b[39minputs: batch_data,\n\u001b[1;32m     16\u001b[0m         model\u001b[38;5;241m.\u001b[39mis_training: \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     }\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/nas/docker/thesis/dataloader/ts_reader.py:36\u001b[0m, in \u001b[0;36mLSTMTSDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# **load time series**\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mHDFStore(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeseries.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m store:\n\u001b[0;32m---> 36\u001b[0m     timeseries \u001b[38;5;241m=\u001b[39m \u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/table\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloc[patient_id] \n\u001b[1;32m     37\u001b[0m     ts_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(timeseries) \n\u001b[1;32m     38\u001b[0m     timeseries \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(timeseries\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/pandas/io/pytables.py:813\u001b[0m, in \u001b[0;36mHDFStore.get\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo object named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 813\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/pandas/io/pytables.py:1880\u001b[0m, in \u001b[0;36mHDFStore._read_group\u001b[0;34m(self, group)\u001b[0m\n\u001b[1;32m   1878\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_storer(group)\n\u001b[1;32m   1879\u001b[0m s\u001b[38;5;241m.\u001b[39minfer_axes()\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/pandas/io/pytables.py:3292\u001b[0m, in \u001b[0;36mBlockManagerFixed.read\u001b[0;34m(self, where, columns, start, stop)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnblocks):\n\u001b[1;32m   3291\u001b[0m     blk_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_index(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_items\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 3292\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mblock\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_values\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_stop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3294\u001b[0m     columns \u001b[38;5;241m=\u001b[39m items[items\u001b[38;5;241m.\u001b[39mget_indexer(blk_items)]\n\u001b[1;32m   3295\u001b[0m     df \u001b[38;5;241m=\u001b[39m DataFrame(values\u001b[38;5;241m.\u001b[39mT, columns\u001b[38;5;241m=\u001b[39mcolumns, index\u001b[38;5;241m=\u001b[39maxes[\u001b[38;5;241m1\u001b[39m], copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/pandas/io/pytables.py:2965\u001b[0m, in \u001b[0;36mGenericFixed.read_array\u001b[0;34m(self, key, start, stop)\u001b[0m\n\u001b[1;32m   2963\u001b[0m     ret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   2964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2965\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mnode\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2967\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mand\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatetime64\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   2968\u001b[0m     \u001b[38;5;66;03m# reconstruct a timezone if indicated\u001b[39;00m\n\u001b[1;32m   2969\u001b[0m     tz \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(attrs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/tables/array.py:650\u001b[0m, in \u001b[0;36mArray.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;66;03m# First, try with a regular selection\u001b[39;00m\n\u001b[1;32m    649\u001b[0m     startl, stopl, stepl, shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interpret_indexing(key)\n\u001b[0;32m--> 650\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstartl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    652\u001b[0m     \u001b[38;5;66;03m# Then, try with a point-wise selection\u001b[39;00m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/tables/array.py:758\u001b[0m, in \u001b[0;36mArray._read_slice\u001b[0;34m(self, startl, stopl, stepl, shape)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;66;03m# Protection against reading empty arrays\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m shape:\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;66;03m# Arrays that have non-zero dimensionality\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_g_read_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstartl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstepl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnparr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[38;5;66;03m# For zero-shaped arrays, return the scalar\u001b[39;00m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nparr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m ():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    for (flats, seqs_padded, seq_lengths), labels, ids in lstm_loader:\n",
    "        # 将 PyTorch Tensor 转换为 numpy 数组\n",
    "        # 注意：如果你的 tensor 在 GPU 上，则调用 .cpu().numpy()\n",
    "        batch_data = seqs_padded.cpu().numpy() if seqs_padded.is_cuda else seqs_padded.numpy()\n",
    "        \n",
    "        feed_dict = {\n",
    "            model.inputs: batch_data,\n",
    "            model.is_training: True\n",
    "        }\n",
    "        # 执行一次训练步骤并获得当前 loss\n",
    "        _, loss_val = sess.run([model.optimize, model.loss], feed_dict=feed_dict)\n",
    "        epoch_loss += loss_val\n",
    "        num_batches += 1\n",
    "    print(\"Epoch {}: Average Loss = {:.4f}\".format(epoch+1, epoch_loss/num_batches))\n",
    "    \n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在训练结束后，将 is_training 设为 False，对一个 batch 进行前向传播，提取 SOM 坐标以进行可视化\n",
    "feed_dict = {model.inputs: data[0:batch_size], model.is_training: False}\n",
    "x_recon_dist_val, z_seq_val, pred_dist_val, som_coords_val = sess.run(\n",
    "    [model.reconstruction_e, tf.reshape(model.z_e_sample, [model.batch_size, model.step_size, model.latent_dim]),\n",
    "     model.prediction, \n",
    "     # SOM 坐标计算部分在 forward 中已经实现\n",
    "     tf.reshape(tf.stack([tf.cast(tf.div(tf.argmin(tf.reduce_sum(tf.square(tf.expand_dims(tf.reshape(model.z_e_sample, [model.batch_size, model.step_size, model.latent_dim]), 0)\n",
    "                        - tf.reshape(model.embeddings, [-1, model.latent_dim])), axis=-1), model.som_dim[1]), tf.float32),\n",
    "                   tf.cast(tf.mod(tf.argmin(tf.reduce_sum(tf.square(tf.expand_dims(tf.reshape(model.z_e_sample, [model.batch_size, model.step_size, model.latent_dim]), 0)\n",
    "                        - tf.reshape(model.embeddings, [-1, model.latent_dim])), axis=-1), model.som_dim[1]), tf.float32)\n",
    "                  ], axis=1), [model.batch_size, model.step_size, 2])\n",
    "    ],\n",
    "    feed_dict=feed_dict\n",
    ")\n",
    "\n",
    "# 这里直接使用模型 forward 中计算的 SOM 坐标，若 forward 返回 som_coords 则直接提取：\n",
    "# 假设 som_coords_val 为 [batch_size, step_size, 2]\n",
    "som_coords_val = sess.run(model.forward(data[0:batch_size])[3], feed_dict={model.is_training: False})\n",
    "print(\"SOM 坐标形状:\", som_coords_val.shape)\n",
    "\n",
    "# 对 batch 中第一个样本的 SOM 坐标轨迹进行可视化\n",
    "import matplotlib.pyplot as plt\n",
    "som_traj = som_coords_val[0]  # shape: [step_size, 2]\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(som_traj[:, 0], som_traj[:, 1], marker='o', linestyle='-')\n",
    "for i, (x_val, y_val) in enumerate(som_traj):\n",
    "    plt.text(x_val, y_val, str(i), fontsize=8)\n",
    "plt.xlabel(\"SOM Grid X\")\n",
    "plt.ylabel(\"SOM Grid Y\")\n",
    "plt.title(\"病人健康状态在 SOM 网格中的轨迹\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eicu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
