{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "sys.path.append('/home/mei/nas/docker/thesis')\n",
    "from model.lstm_gnn_embedding import PatientOutcomeModelEmbedding\n",
    "from dataloader.ts_reader import MultiModalDataset, collate_fn\n",
    "from dataloader.pyg_reader import GraphDataset\n",
    "from captum.attr import IntegratedGradients\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"/home/mei/nas/docker/thesis/data/hdf/train\"\n",
    "val_data_dir = \"/home/mei/nas/docker/thesis/data/hdf/val\"\n",
    "test_data_dir = \"/home/mei/nas/docker/thesis/data/hdf/test\"\n",
    "\n",
    "config = {  \n",
    "    \"data_dir\": \"/home/mei/nas/docker/thesis/data/hdf\",\n",
    "    \"graph_dir\": \"/home/mei/nas/docker/thesis/data/graphs\",\n",
    "    \"mode\": \"k_closest\",\n",
    "    \"k\": 3         \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Loading precomputed graph from /home/mei/nas/docker/thesis/data/graphs/diagnosis_graph_k_closest_k3.pt\n",
      "==> Loading flat features from /home/mei/nas/docker/thesis/data/hdf/final_flat.h5\n"
     ]
    }
   ],
   "source": [
    "# === LSTM + Flat Dataset ===\n",
    "lstm_dataset_train = MultiModalDataset(train_data_dir)\n",
    "lstm_dataset_val = MultiModalDataset(val_data_dir)\n",
    "lstm_dataset_test = MultiModalDataset(test_data_dir)\n",
    "\n",
    "lstm_loader_train = DataLoader(lstm_dataset_train , batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "lstm_loader_val = DataLoader(lstm_dataset_val , batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "lstm_loader_test = DataLoader(lstm_dataset_test , batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# === Graph Dataset ===\n",
    "\n",
    "graph_dataset = GraphDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_data= pd.read_hdf(test_data_dir+'/flat.h5')\n",
    "ts_data= pd.read_hdf(test_data_dir+'/timeseries.h5')\n",
    "feature_flat = list(flat_data.columns)\n",
    "feature_ts = list(ts_data.columns)\n",
    "feature_ts = feature_ts[1:] # remove 'time' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_input_dim = 104\n",
    "graph_input_dim = 104\n",
    "ts_input_dim = 162\n",
    "hidden_dim = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")   \n",
    "model = PatientOutcomeModelEmbedding(flat_input_dim, graph_input_dim, ts_input_dim, hidden_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dataset.graph_data.x = graph_dataset.graph_data.x.to(device)\n",
    "graph_dataset.graph_data.edge_index = graph_dataset.graph_data.edge_index.to(device)\n",
    "graph_dataset.graph_data.patient_ids =graph_dataset.graph_data.patient_ids.clone().detach().to(device)\n",
    "\n",
    "for patient_ids, ts_data, flat_data, risk_data, lengths in lstm_loader_val:\n",
    "    fixed_patient_ids = torch.tensor([int(pid) for pid in patient_ids], dtype=torch.long).to(device)  # shape: (N,)\n",
    "    fixed_ts = ts_data.to(device)  # shape: (N, T, D_ts)\n",
    "    fixed_lengths = lengths.to(device)  # shape: (N,)\n",
    "    fixed_flat = flat_data.to(device)  # shape: (N, D_flat)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapperForIG(nn.Module):\n",
    "    def __init__(self, model, graph_data, patient_ids_fixed, ts_data_fixed, lengths_fixed):\n",
    "        super(ModelWrapperForIG, self).__init__()\n",
    "        self.model = model\n",
    "        self.graph_data = graph_data\n",
    "        self.patient_ids_fixed = patient_ids_fixed  \n",
    "        self.ts_data_fixed = ts_data_fixed  \n",
    "        self.lengths_fixed = lengths_fixed  \n",
    "        \n",
    "    def forward(self, flat_input, ts_input):\n",
    "        \"\"\"\n",
    "        flat_input: tensor, shape (N, D_flat)\n",
    "        ts_input: tensor, shape (N, T, D_ts)\n",
    "        其他输入固定：graph_data, patient_ids_fixed, ts_data_fixed, lengths_fixed\n",
    "        这里我们将调用模型并返回每个样本 risk score 取所有时间步均\n",
    "        \"\"\"\n",
    "        risk_scores, _ = self.model(flat_input, self.graph_data, self.patient_ids_fixed, ts_input, self.lengths_fixed)\n",
    "        # risk_scores shape: (N, T) -> 返回每个样本在时间维度上的均值\n",
    "        return risk_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelWrapperForIG(\n",
       "  (model): PatientOutcomeModelEmbedding(\n",
       "    (flat_encoder): Linear(in_features=104, out_features=128, bias=True)\n",
       "    (graph_encoder): GraphEncoder(\n",
       "      (gcn1): GCNConv(104, 128)\n",
       "      (gcn2): GCNConv(128, 128)\n",
       "      (bn1): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (ts_encoder): TimeSeriesEncoder(\n",
       "      (lstm): LSTM(162, 128, batch_first=True, bidirectional=True)\n",
       "    )\n",
       "    (risk_predictor): RiskPredictor(\n",
       "      (fc1): Linear(in_features=512, out_features=128, bias=True)\n",
       "      (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapper = ModelWrapperForIG(model, graph_dataset.graph_data, fixed_patient_ids, fixed_ts, fixed_lengths)\n",
    "wrapper.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_flat(flat_input):\n",
    "    \"\"\"\n",
    "    解释平面特征：\n",
    "    flat_input: tensor, shape (N, D_flat)\n",
    "    固定 ts_input 使用 ts_sample（从测试集中选取的样本的时序部分）\n",
    "    \"\"\"\n",
    "    # 在这里，我们固定 ts_input 为测试样本 ts_sample（稍后获取）\n",
    "    risk_scores = wrapper(flat_input, ts_sample)\n",
    "    return risk_scores\n",
    "\n",
    "def forward_ts(ts_input):\n",
    "    \"\"\"\n",
    "    解释时序特征：\n",
    "    ts_input: tensor, shape (N, T, D_ts)\n",
    "    固定平面输入为 flat_sample（从测试集中选取的样本的平面部分）\n",
    "    \"\"\"\n",
    "    risk_scores = wrapper(flat_sample, ts_input)\n",
    "    return risk_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_sample shape: torch.Size([32, 3995, 162])\n",
      "baseline_ts shape: torch.Size([32, 3995, 162])\n"
     ]
    }
   ],
   "source": [
    "print(\"ts_sample shape:\", ts_sample.shape)\n",
    "print(\"baseline_ts shape:\", baseline_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[36, 1, 256]' is invalid for input of size 2560",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 对于平面特征 Integrated Gradients，不设置 target 参数\u001b[39;00m\n\u001b[1;32m     13\u001b[0m ig_flat \u001b[38;5;241m=\u001b[39m IntegratedGradients(forward_flat)\n\u001b[0;32m---> 14\u001b[0m attr_flat, delta_flat \u001b[38;5;241m=\u001b[39m \u001b[43mig_flat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_convergence_delta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlat attribution:\u001b[39m\u001b[38;5;124m\"\u001b[39m, attr_flat)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFlat convergence delta:\u001b[39m\u001b[38;5;124m\"\u001b[39m, delta_flat)\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/captum/log/__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/captum/attr/_core/integrated_gradients.py:286\u001b[0m, in \u001b[0;36mIntegratedGradients.attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta)\u001b[0m\n\u001b[1;32m    274\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m _batch_attribution(\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    276\u001b[0m         num_examples,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    283\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     attributions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaselines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaselines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_convergence_delta:\n\u001b[1;32m    296\u001b[0m     start_point, end_point \u001b[38;5;241m=\u001b[39m baselines, inputs\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/captum/attr/_core/integrated_gradients.py:351\u001b[0m, in \u001b[0;36mIntegratedGradients._attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    348\u001b[0m expanded_target \u001b[38;5;241m=\u001b[39m _expand_target(target, n_steps)\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# grads: dim -> (bsz * #steps x inputs[0].shape[1:], ...)\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaled_features_tpl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_additional_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# flattening grads so that we can multilpy it with step-size\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;66;03m# calling contiguous to avoid `memory whole` problems\u001b[39;00m\n\u001b[1;32m    360\u001b[0m scaled_grads \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    361\u001b[0m     grad\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(n_steps, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(step_sizes)\u001b[38;5;241m.\u001b[39mview(n_steps, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(grad\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m grad \u001b[38;5;129;01min\u001b[39;00m grads\n\u001b[1;32m    364\u001b[0m ]\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/captum/_utils/gradient.py:112\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[0;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03mComputes gradients of the output with respect to inputs for an\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124;03marbitrary forward function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m                arguments) if no additional arguments are required\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;66;03m# runs forward pass\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43m_run_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforward_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget not provided when necessary, cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m take gradient with respect to multiple outputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;66;03m# contains batch_size * #steps elements\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/captum/_utils/common.py:531\u001b[0m, in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    528\u001b[0m inputs \u001b[38;5;241m=\u001b[39m _format_inputs(inputs)\n\u001b[1;32m    529\u001b[0m additional_forward_args \u001b[38;5;241m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[0;32m--> 531\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mforward_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madditional_forward_args\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _select_targets(output, target)\n",
      "Cell \u001b[0;32mIn[75], line 8\u001b[0m, in \u001b[0;36mforward_flat\u001b[0;34m(flat_input)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m解释平面特征：\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mflat_input: tensor, shape (N, D_flat)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m固定 ts_input 使用 ts_sample（从测试集中选取的样本的时序部分）\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 在这里，我们固定 ts_input 为测试样本 ts_sample（稍后获取）\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m risk_scores \u001b[38;5;241m=\u001b[39m \u001b[43mwrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m risk_scores\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[69], line 17\u001b[0m, in \u001b[0;36mModelWrapperForIG.forward\u001b[0;34m(self, flat_input, ts_input)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, flat_input, ts_input):\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    flat_input: tensor, shape (N, D_flat)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    ts_input: tensor, shape (N, T, D_ts)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    其他输入固定：graph_data, patient_ids_fixed, ts_data_fixed, lengths_fixed\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    这里我们将调用模型并返回每个样本 risk score（取所有时间步均值）\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     risk_scores, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatient_ids_fixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlengths_fixed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# risk_scores shape: (N, T) -> 返回每个样本在时间维度上的均值\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m risk_scores\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nas/docker/thesis/model/lstm_gnn_embedding.py:101\u001b[0m, in \u001b[0;36mPatientOutcomeModelEmbedding.forward\u001b[0;34m(self, flat_data, graph_data, patient_ids, ts_data, lengths)\u001b[0m\n\u001b[1;32m     97\u001b[0m flat_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflat_encoder(flat_data)  \u001b[38;5;66;03m# (batch_size, D_flat)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# print(f\"flat encoder output: flat_emb.shape = {flat_emb.shape}\")\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# === calculate Time-Series Embeddings ===\u001b[39;00m\n\u001b[0;32m--> 101\u001b[0m ts_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mts_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch_size, T, D_ts)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# print(f\"Time series encoder output: ts_emb.shape = {ts_emb.shape}\")\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# === calculate Risk Scores ===\u001b[39;00m\n\u001b[1;32m    105\u001b[0m risk_scores,combimed_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrisk_predictor(flat_emb, batch_graph_embeddings, ts_emb)\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/nas/docker/thesis/model/lstm_gnn_embedding.py:42\u001b[0m, in \u001b[0;36mTimeSeriesEncoder.forward\u001b[0;34m(self, x, lengths)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# print(f\"packed sequence batch_sizes = {packed.batch_sizes}\") # print the batch_sizes\u001b[39;00m\n\u001b[1;32m     41\u001b[0m packed_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(packed)\n\u001b[0;32m---> 42\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[43mpad_packed_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpacked_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m99\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# print(f\"Time series Encoder output: out.shape = {out.shape}\")\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/eicu/lib/python3.10/site-packages/torch/nn/utils/rnn.py:333\u001b[0m, in \u001b[0;36mpad_packed_sequence\u001b[0;34m(sequence, batch_first, padding_value, total_length)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected total_length to be at least the length \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    329\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof the longest sequence in input, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    330\u001b[0m                          \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_length=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and max sequence length being \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_seq_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    331\u001b[0m                          )\n\u001b[1;32m    332\u001b[0m     max_seq_length \u001b[38;5;241m=\u001b[39m total_length\n\u001b[0;32m--> 333\u001b[0m padded_output, lengths \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pad_packed_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m unsorted_indices \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39munsorted_indices\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unsorted_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[36, 1, 256]' is invalid for input of size 2560"
     ]
    }
   ],
   "source": [
    "for patient_ids, ts_data, flat_data, risk_data, lengths in lstm_loader_test:\n",
    "    flat_sample = flat_data.to(device)       # shape: (batch_size, D_flat)\n",
    "    ts_sample = ts_data.to(device)           # shape: (batch_size, T, D_ts)\n",
    "    # 为 Integrated Gradients 选择基线，通常使用全零\n",
    "    baseline_flat = torch.zeros_like(flat_sample)\n",
    "    baseline_ts = torch.zeros_like(ts_sample)\n",
    "    break\n",
    "\n",
    "\n",
    "ig_flat = IntegratedGradients(forward_flat)\n",
    "\n",
    "# 对于平面特征 Integrated Gradients，不设置 target 参数\n",
    "ig_flat = IntegratedGradients(forward_flat)\n",
    "attr_flat, delta_flat = ig_flat.attribute(flat_sample, baseline_flat, return_convergence_delta=True)\n",
    "print(\"Flat attribution:\", attr_flat)\n",
    "print(\"Flat convergence delta:\", delta_flat)\n",
    "\n",
    "# 对于时序特征 Integrated Gradients，也不设置 target 参数\n",
    "ig_ts = IntegratedGradients(forward_ts)\n",
    "attr_ts, delta_ts = ig_ts.attribute(ts_sample, baseline_ts, return_convergence_delta=True)\n",
    "print(\"Time series attribution:\", attr_ts)\n",
    "print(\"Time series convergence delta:\", delta_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_flat_np = attr_flat.mean(dim=0).cpu().detach().numpy()  # (D_flat,)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(attr_flat_np)), attr_flat_np)\n",
    "plt.xlabel(\"Flat Feature Index\")\n",
    "plt.ylabel(\"Attribution\")\n",
    "plt.title(\"Integrated Gradients for Flat Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_ts_np = attr_ts.mean(dim=0).cpu().detach().numpy()  # (T, D_ts)\n",
    "# 例如，我们取每个时间步所有特征的均值\n",
    "time_step_attr = attr_ts_np.mean(axis=1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(time_step_attr, marker='o')\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Mean Attribution\")\n",
    "plt.title(\"Integrated Gradients for Time Series Features (Averaged over Features)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapperForIG(torch.nn.Module):\n",
    "    def __init__(self, model, graph_data, patient_ids_fixed, ts_data_fixed, lengths_fixed):\n",
    "        \"\"\"\n",
    "        model: OutcomeModelEmbedding model, set it to eval mode\n",
    "        graph_data: fixed graph data\n",
    "        patient_ids_fixed: shape (N,)\n",
    "        ts_data_fixed: shape (N, T, D_ts)\n",
    "        lengths_fixed: the valid lengths of each time series in ts_data_fixed\n",
    "        \"\"\"\n",
    "        super(ModelWrapperForIG, self).__init__()\n",
    "        self.model = model\n",
    "        self.graph_data = graph_data\n",
    "        self.patient_ids_fixed = torch.tensor(patient_ids_fixed, dtype=torch.long).to(model.device)  # Ensure it's a tensor and on the same device\n",
    "        self.ts_data_fixed = ts_data_fixed.to(model.device)\n",
    "        self.lengths_fixed = lengths_fixed.to(model.device)\n",
    "        self.device = self.patient_ids_fixed.device  # Get the device\n",
    "\n",
    "    def forward(self, flat_input, ts_input):\n",
    "        \"\"\"\n",
    "        flat_input: tensor, shape (N, D_flat)\n",
    "        ts_input: tensor, shape (N, T, D_ts)\n",
    "        this function should return the risk scores for each sample in the batch\n",
    "        \"\"\"\n",
    "        # get risk_scores and combined embeddings\n",
    "        risk_scores, combined = self.model(flat_input, self.graph_data, self.patient_ids_fixed, ts_input, self.lengths_fixed)\n",
    "        # risk_scores shape is (N, T)\n",
    "        risk_mean = risk_scores.mean(dim=1)  # shape (N,)\n",
    "        return risk_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient_ids, ts_data, flat_data, risk_data ,lengths in lstm_loader_val:\n",
    "    patient_ids_fixed = patient_ids  # Ensure it's a list\n",
    "    ts_data_fixed = ts_data\n",
    "    lengths_fixed = lengths\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ts_data_fixed = ts_data_fixed.to(device)\n",
    "lengths_fixed = lengths_fixed.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wrapper \u001b[38;5;241m=\u001b[39m \u001b[43mModelWrapperForIG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_ids_fixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts_data_fixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths_fixed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m wrapper\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m ig \u001b[38;5;241m=\u001b[39m IntegratedGradients(wrapper)\n",
      "Cell \u001b[0;32mIn[25], line 13\u001b[0m, in \u001b[0;36mModelWrapperForIG.__init__\u001b[0;34m(self, model, graph_data, patient_ids_fixed, ts_data_fixed, lengths_fixed)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph_data \u001b[38;5;241m=\u001b[39m graph_data\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatient_ids_fixed \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatient_ids_fixed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Ensure it's a tensor and on the same device\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mts_data_fixed \u001b[38;5;241m=\u001b[39m ts_data_fixed\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlengths_fixed \u001b[38;5;241m=\u001b[39m lengths_fixed\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "wrapper = ModelWrapperForIG(model, graph_dataset.graph_data, patient_ids_fixed, ts_data_fixed, lengths_fixed)\n",
    "wrapper.eval()\n",
    "\n",
    "ig = IntegratedGradients(wrapper)\n",
    "\n",
    "# 选择一个输入样本和基线数据进行解释\n",
    "for patient_ids, ts_data, flat_data, risk_data in lstm_loader_test:\n",
    "    flat_input = flat_data.to(device)\n",
    "    ts_sample = ts_data.to(device)\n",
    "    baseline_ts = torch.zeros_like(ts_sample).to(device)  # 使用全零作为基线数据\n",
    "    break\n",
    "\n",
    "\n",
    "attr_ts, delta_ts = ig.attribute((flat_input, ts_sample), (flat_input, baseline_ts), target=0, return_convergence_delta=True)\n",
    "print(\"Time series attribution:\", attr_ts)\n",
    "print(\"Time series convergence delta:\", delta_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_ts = attr_ts[1].cpu().detach().numpy()  # 获取时间序列部分的归因\n",
    "ts_sample = ts_sample.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.mean(attr_ts, axis=0))\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Attribution')\n",
    "plt.title('Time Series Feature Importance using Integrated Gradients')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eicu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
