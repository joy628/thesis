{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5bb450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.append('/home/mei/nas/docker/thesis/model_train')\n",
    "from dataloader.ts_reader import MultiModalDataset,VitalSignsDataset,vital_pre_train\n",
    "from model.auencoder_v2 import *\n",
    "from model.autoencoder_v2_loss_train import *\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "gc.collect()\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5a40e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vital_signs_train = '/home/mei/nas/docker/thesis/data/hdf/train/ts_each_patient.h5'\n",
    "vital_signs_val = '/home/mei/nas/docker/thesis/data/hdf/val/ts_each_patient.h5'\n",
    "vital_signs_test = '/home/mei/nas/docker/thesis/data/hdf/test/ts_each_patient.h5'\n",
    "\n",
    "train_dataset = VitalSignsDataset(vital_signs_train)\n",
    "val_dataset = VitalSignsDataset(vital_signs_val)\n",
    "test_dataset = VitalSignsDataset(vital_signs_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=vital_pre_train)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=vital_pre_train) \n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=vital_pre_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c79b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model  = PatientAutoencoder(\n",
    "    n_features=154, hidden_dim=128, latent_dim=100,\n",
    "                 grid_size=(10,10), lstm_hidden_dim=128, n_heads=2\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4512177",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/mei/nas/docker/thesis/data/model_results/autoencoder\"\n",
    "stage1_dir = os.path.join(base_dir, \"stage1\")\n",
    "os.makedirs(stage1_dir, exist_ok=True)\n",
    "\n",
    "pretrain_epochs  = 100\n",
    "prior_max        = 1e-5\n",
    "anneal_steps     = 200\n",
    "opt_vae = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "hist_pre = train_vae_pretrain(\n",
    "    model,\n",
    "    loader=train_loader,\n",
    "    optimizer=opt_vae,\n",
    "    device=device,\n",
    "    checkpoint_dir=stage1_dir,\n",
    "    epochs=pretrain_epochs,\n",
    "    prior_max=prior_max,\n",
    "    anneal_steps=anneal_steps\n",
    ")\n",
    "\n",
    "with open(os.path.join(stage1_dir, \"history_pretrain.json\"), \"w\") as f:\n",
    "    json.dump(hist_pre, f, indent=2)\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    os.path.join(stage1_dir, \"best_model_vae.pth\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage2_dir = os.path.join(base_dir, \"stage2\")\n",
    "os.makedirs(stage2_dir, exist_ok=True)\n",
    "\n",
    "som_epochs_stage = 50//3\n",
    "som_lrs=(0.1,0.01,0.001)\n",
    "grid_size=(10,10)\n",
    "\n",
    "\n",
    "hist_som = init_som_centroids(\n",
    "    model,\n",
    "    loader=train_loader,\n",
    "    device=device,\n",
    "    checkpoint_dir=stage2_dir,\n",
    "    epochs_per_stage=som_epochs_stage,\n",
    "    lrs=som_lrs,\n",
    "    grid_size=grid_size\n",
    ")\n",
    "\n",
    "with open(os.path.join(stage2_dir, \"history_som_init.json\"), \"w\") as f:\n",
    "    json.dump(hist_som, f, indent=2)\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    os.path.join(stage2_dir, \"best_model_som_init.pth\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30fd481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Joint] Epoch 10/10  train=127.1459  val=122.9113\n",
      "  ➡ saved /home/mei/nas/docker/thesis/data/model_results/autoencoder/stage3/joint_epoch010.pth\n"
     ]
    }
   ],
   "source": [
    "joint_epochs=100\n",
    "base_lr=1e-3\n",
    "decay=0.99\n",
    "weights = {\n",
    "        'recon': 1.0, 'kl': 1.0, 'cluster': 1.0,\n",
    "        'ssom': 10.0, 'smooth': 1.0, 'pred': 1.0,\n",
    "        'cluster_k': 2.0\n",
    "    }\n",
    "\n",
    "\n",
    "stage3_dir = os.path.join(base_dir, \"stage3\")\n",
    "os.makedirs(stage3_dir, exist_ok=True)\n",
    "\n",
    "hist_joint = joint_train(\n",
    "    model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    checkpoint_dir=stage3_dir,\n",
    "    epochs=joint_epochs,\n",
    "    base_lr=base_lr,\n",
    "    decay=decay,\n",
    "    weights=weights,\n",
    "    grid_size=(10,10)\n",
    ")\n",
    "\n",
    "with open(os.path.join(stage3_dir, \"history_joint_train.json\"), \"w\") as f:\n",
    "    json.dump(hist_joint, f, indent=2)\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    os.path.join(stage3_dir, \"best_model_joint.pth\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c48201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pred Finetune] ep 5/5  loss=0.4412\n",
      "  ➡ saved /home/mei/nas/docker/thesis/data/model_results/autoencoder/stage4/predfinetune_epoch005.pth\n"
     ]
    }
   ],
   "source": [
    "pred_epochs = 50\n",
    "\n",
    "stage4_dir = os.path.join(base_dir, \"stage4\")\n",
    "os.makedirs(stage4_dir, exist_ok=True)\n",
    "\n",
    "hist_pred = predict_finetune(\n",
    "    model,\n",
    "    loader=train_loader,\n",
    "    device=device,\n",
    "    checkpoint_dir=stage4_dir,\n",
    "    epochs=pred_epochs\n",
    ")\n",
    "\n",
    "with open(os.path.join(stage4_dir, \"history_pred_finetune.json\"), \"w\") as f:\n",
    "    json.dump(hist_pred, f, indent=2)\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    os.path.join(stage4_dir, \"best_model_pred.pth\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_stage_losses(hist_pre, hist_som, hist_joint, hist_pred):\n",
    "    \"\"\"\n",
    "    绘制四个训练阶段的 loss 曲线：\n",
    "      - hist_pre   : VAE 预训练阶段 loss list\n",
    "      - hist_som   : SOM centroid 初始化阶段 loss list\n",
    "      - hist_joint : 联合训练阶段 loss list\n",
    "      - hist_pred  : 预测微调阶段 loss list\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "    \n",
    "    # Stage 1: VAE Pretrain\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(range(1, len(hist_pre) + 1), hist_pre, marker='o')\n",
    "    ax.set_title(\"Stage1: VAE Pretrain\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Stage 2: SOM Init\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(range(1, len(hist_som) + 1), hist_som, color='orange', marker='o')\n",
    "    ax.set_title(\"Stage2: SOM Init Centroids\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Stage 3: Joint Train\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(range(1, len(hist_joint[\"train\"]) + 1), hist_joint[\"train\"], label='Train')\n",
    "    ax.plot(range(1, len(hist_joint[\"val\"]  ) + 1), hist_joint[\"val\"],   label='Val', linestyle='--')\n",
    "    ax.set_title(\"Stage3: Joint Train\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    \n",
    "    # Stage 4: Prediction Finetune\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(range(1, len(hist_pred) + 1), hist_pred, color='green', marker='o')\n",
    "    ax.set_title(\"Stage4: Predict Finetune\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecad6e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_stage_losses(hist_pre, hist_som, hist_joint, hist_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b591202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/mei/nas/docker/thesis/data/hdf/features.txt', 'r') as f:\n",
    "    features = [line.strip() for line in f]\n",
    "\n",
    "selected_feature_indices = [0,1,2,142,143,144,145,146]  #\n",
    "\n",
    "visualize_recons(model, test_loader, num_patients=5, feature_indices=selected_feature_indices, feature_names=features,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f68f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
